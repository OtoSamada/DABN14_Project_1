{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_csv('../data/True.csv')\n",
    "fake_df = pd.read_csv('../data/Fake.csv')\n",
    "\n",
    "# Create mapping for subject categories\n",
    "subject_mapping = {\n",
    "    # Real news categories\n",
    "    'politicsNews': 'Politics',\n",
    "    'worldnews': 'WorldNews',\n",
    "    \n",
    "    # Fake news categories\n",
    "    'politics': 'Politics',\n",
    "    'Government News': 'Politics',\n",
    "    'US_News': 'Politics',\n",
    "    'left-news': 'Politics',\n",
    "    'News': 'WorldNews',\n",
    "    'Middle-east': 'WorldNews'\n",
    "}\n",
    "\n",
    "# Apply mapping to both dataframes\n",
    "real_df['subject'] = real_df['subject'].map(subject_mapping)\n",
    "fake_df['subject'] = fake_df['subject'].map(subject_mapping)\n",
    "\n",
    "# Verify the mapping\n",
    "print(\"Real news subjects after mapping:\")\n",
    "print(real_df['subject'].value_counts())\n",
    "print(\"\\nFake news subjects after mapping:\")\n",
    "print(fake_df['subject'].value_counts())\n",
    "\n",
    "# Add label column to distinguish real vs fake news\n",
    "real_df['label'] = 1  # Real news\n",
    "fake_df['label'] = 0  # Fake news\n",
    "\n",
    "# Union the dataframes\n",
    "df = pd.concat([real_df, fake_df], ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {df.shape}\")\n",
    "\n",
    "# Remove Reuters-style prefixes from text (e.g., \"WASHINGTON (Reuters) - \")\n",
    "def remove_reuters_prefix(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Pattern matches: CITY (Reuters) - or CITY, STATE (Reuters) -\n",
    "    # Examples: WASHINGTON (Reuters) -, Beijing (Reuters) -, NEW YORK (Reuters) -\n",
    "    pattern = r'^[A-Z][A-Za-z\\s,]+\\(Reuters\\)\\s*-\\s*'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_reuters_prefix)\n",
    "\n",
    "print(\"Removed Reuters prefixes from text\")\n",
    "\n",
    "# Step 1: Calculate repost_count BEFORE removing duplicates\n",
    "df['repost_count'] = df.groupby(['title', 'text'])['title'].transform('count')\n",
    "\n",
    "# Step 2: Sort by date and keep earliest (convert date to datetime first)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Remove duplicates, keeping the first (earliest) occurrence\n",
    "df = df.drop_duplicates(subset=['title', 'text'], keep='first')\n",
    "\n",
    "print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "\n",
    "# Initialize emotion classifier with MPS (Metal Performance Shaders) for M3 Max\n",
    "print(\"Loading emotion detection model...\")\n",
    "emotion_classifier = pipeline(\"text-classification\", \n",
    "                              model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "                              top_k=None,\n",
    "                              device=\"mps\",  # Use Metal GPU on Mac\n",
    "                              batch_size=128,\n",
    "                              truncation=True,\n",
    "                              max_length=512)\n",
    "\n",
    "# Add Stylometric Features for TEXT\n",
    "def capital_ratio(text):\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return 0\n",
    "    letters = [c for c in text if c.isalpha()]\n",
    "    if len(letters) == 0:\n",
    "        return 0\n",
    "    return sum(1 for c in letters if c.isupper()) / len(letters)\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return 0\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if len(sentences) == 0:\n",
    "        return 0\n",
    "    total_words = sum(len(s.split()) for s in sentences)\n",
    "    return total_words / len(sentences)\n",
    "\n",
    "df['text_exclamation_count'] = df['text'].apply(lambda x: x.count('!') if isinstance(x, str) else 0)\n",
    "df['text_capital_ratio'] = df['text'].apply(capital_ratio)\n",
    "df['text_avg_sentence_length'] = df['text'].apply(avg_sentence_length)\n",
    "\n",
    "# Add Stylometric Features for TITLE\n",
    "df['title_exclamation_count'] = df['title'].apply(lambda x: x.count('!') if isinstance(x, str) else 0)\n",
    "df['title_capital_ratio'] = df['title'].apply(capital_ratio)\n",
    "\n",
    "# Add Sentiment Features using TextBlob\n",
    "def get_sentiment_features(text):\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return 0, 0\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "df['text_sentiment_polarity'] = df['text'].apply(lambda x: get_sentiment_features(x)[0])\n",
    "df['text_sentiment_subjectivity'] = df['text'].apply(lambda x: get_sentiment_features(x)[1])\n",
    "df['title_sentiment_polarity'] = df['title'].apply(lambda x: get_sentiment_features(x)[0])\n",
    "df['title_sentiment_subjectivity'] = df['title'].apply(lambda x: get_sentiment_features(x)[1])\n",
    "\n",
    "# Optimized batch emotion detection function\n",
    "def get_emotion_features_batch(texts):\n",
    "    \"\"\"Process texts in batches for much faster inference\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Filter out invalid texts and track indices\n",
    "    valid_texts = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "        if isinstance(text, str) and len(text) > 0:\n",
    "            valid_texts.append(text)\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    # Process all valid texts at once\n",
    "    if valid_texts:\n",
    "        try:\n",
    "            batch_emotions = emotion_classifier(valid_texts)\n",
    "            \n",
    "            # Map results back to original indices\n",
    "            emotion_dict = {}\n",
    "            for i, emotions in enumerate(batch_emotions):\n",
    "                top_emotion = max(emotions, key=lambda x: x['score'])\n",
    "                emotional_intensity = max([e['score'] for e in emotions])\n",
    "                \n",
    "                emotion_dict[valid_indices[i]] = (\n",
    "                    top_emotion['label'],\n",
    "                    top_emotion['score'],\n",
    "                    emotional_intensity\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Batch processing error: {e}\")\n",
    "            emotion_dict = {}\n",
    "    else:\n",
    "        emotion_dict = {}\n",
    "    \n",
    "    # Build results list with defaults for invalid texts\n",
    "    for idx in range(len(texts)):\n",
    "        if idx in emotion_dict:\n",
    "            results.append(emotion_dict[idx])\n",
    "        else:\n",
    "            results.append(('neutral', 0.0, 0.0))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process TEXT emotions in batches\n",
    "print(\"Analyzing emotions in text (using batch processing)...\")\n",
    "text_list = df['text'].tolist()\n",
    "batch_size = 128\n",
    "all_text_emotions = []\n",
    "\n",
    "for i in range(0, len(text_list), batch_size):\n",
    "    batch = text_list[i:i+batch_size]\n",
    "    batch_results = get_emotion_features_batch(batch)\n",
    "    all_text_emotions.extend(batch_results)\n",
    "    \n",
    "    if (i // batch_size + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + len(batch)}/{len(text_list)} texts...\")\n",
    "\n",
    "df['text_emotion'] = [x[0] for x in all_text_emotions]\n",
    "df['text_emotion_score'] = [x[1] for x in all_text_emotions]\n",
    "df['text_emotional_intensity'] = [x[2] for x in all_text_emotions]\n",
    "\n",
    "# Process TITLE emotions in batches\n",
    "print(\"Analyzing emotions in titles (using batch processing)...\")\n",
    "title_list = df['title'].tolist()\n",
    "all_title_emotions = []\n",
    "\n",
    "for i in range(0, len(title_list), batch_size):\n",
    "    batch = title_list[i:i+batch_size]\n",
    "    batch_results = get_emotion_features_batch(batch)\n",
    "    all_title_emotions.extend(batch_results)\n",
    "    \n",
    "    if (i // batch_size + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + len(batch)}/{len(title_list)} titles...\")\n",
    "\n",
    "df['title_emotion'] = [x[0] for x in all_title_emotions]\n",
    "df['title_emotion_score'] = [x[1] for x in all_title_emotions]\n",
    "df['title_emotional_intensity'] = [x[2] for x in all_title_emotions]\n",
    "\n",
    "# Display summary of new features\n",
    "print(\"\\n=== New Features Summary ===\")\n",
    "feature_cols = ['text_exclamation_count', 'text_capital_ratio', 'text_avg_sentence_length',\n",
    "                'text_sentiment_polarity', 'text_sentiment_subjectivity', 'text_emotional_intensity',\n",
    "                'title_exclamation_count', 'title_capital_ratio',\n",
    "                'title_sentiment_polarity', 'title_sentiment_subjectivity', 'title_emotional_intensity',\n",
    "                'repost_count']\n",
    "print(df[feature_cols].describe())\n",
    "\n",
    "# Show emotion distribution\n",
    "print(\"\\n=== Text Emotion Distribution ===\")\n",
    "print(df['text_emotion'].value_counts())\n",
    "print(\"\\n=== Title Emotion Distribution ===\")\n",
    "print(df['title_emotion'].value_counts())\n",
    "\n",
    "# Show comparison between fake and real news\n",
    "print(\"\\n=== Feature Comparison: Fake vs Real News ===\")\n",
    "print(\"\\nFake News (label=0):\")\n",
    "print(df[df['label']==0][feature_cols].mean())\n",
    "print(\"\\nReal News (label=1):\")\n",
    "print(df[df['label']==1][feature_cols].mean())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../data/combined_news_with_features.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57144119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "fake_words = Counter(\" \".join(df[df[\"label\"]==0][\"text\"]).split())\n",
    "real_words = Counter(\" \".join(df[df[\"label\"]==1][\"text\"]).split())\n",
    "\n",
    "exclusive_fake = set(fake_words) - set(real_words)\n",
    "exclusive_real = set(real_words) - set(fake_words)\n",
    "\n",
    "print(\"Fake sample words:\", list(exclusive_fake)[:20])\n",
    "print(\"Real sample words :\", list(exclusive_real)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b733d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(exclusive_fake), len(exclusive_real))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
